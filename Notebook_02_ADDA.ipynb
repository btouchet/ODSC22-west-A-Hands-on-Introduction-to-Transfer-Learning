{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from basemodel import LeNet, LeNetEncoder, Discriminator\n",
    "from sklearn.metrics import accuracy_score\n",
    "from dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lenet Model For Source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = tf.random.uniform(shape=(10, 28, 28, 1))\n",
    "y = model(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LeNet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " LeNetEncoder (LeNetEncoder)  multiple                 926070    \n",
      "                                                                 \n",
      " LeNetClassifier (LeNetClass  multiple                 5010      \n",
      " ifier)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 931,080\n",
      "Trainable params: 931,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LeNetEncoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             multiple                  520       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           multiple                  25050     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  multiple                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  multiple                 0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  900500    \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 926,070\n",
      "Trainable params: 926,070\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LeNetClassifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dropout_1 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  5010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,010\n",
      "Trainable params: 5,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets (source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoder module loads the MNIST and USPS datasets and splits them into train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset mnist\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train) = loader.getData('train')\n",
    "(x_test, y_test) = loader.getData('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Data Iterator for mini-batch training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on source with cross-entopy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_metric = tf.keras.metrics.Mean()\n",
    "m = tf.keras.metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "ce_loss_fn = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "step 0: mean loss = 0.5677 acc=0.8284\n",
      "step 100: mean loss = 0.2717 acc=0.9188\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-d7ddb390f3f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dl2\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dl2\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dl2\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dl2\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    566\u001b[0m   \u001b[0muse_cudnn_on_gpu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m   \u001b[0mdata_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data_format\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m   \u001b[0mshape_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape_n\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m   \u001b[1;31m# We call the gen_nn_ops backprop functions instead of nn_ops backprop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dl2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dl2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dl2\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mshape_n\u001b[1;34m(input, out_type, name)\u001b[0m\n\u001b[0;32m    716\u001b[0m   \"\"\"\n\u001b[0;32m    717\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 718\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape_n\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dl2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mshape_n\u001b[1;34m(input, out_type, name)\u001b[0m\n\u001b[0;32m   9411\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9412\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m-> 9413\u001b[1;33m         _ctx, \"ShapeN\", name, input, \"out_type\", out_type)\n\u001b[0m\u001b[0;32m   9414\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9415\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iterate over epochs.\n",
    "for epoch in range(epochs):\n",
    "    print(\"Start of epoch %d\" % (epoch,))\n",
    "    \n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch) in enumerate(train_dataset):\n",
    "        #x_batch_train = tf.expand_dims(x_batch_train, axis=3)\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred = model(x_batch_train)\n",
    "                \n",
    "            loss = tf.reduce_mean(ce_loss_fn(y_batch, pred))\n",
    "            m.update_state(tf.argmax(y_batch, 1),tf.argmax(pred, 1))\n",
    "    \n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "        loss_metric(loss)\n",
    "    \n",
    "        if step % 100 == 0:\n",
    "            print(\"step %d: mean loss = %.4f acc=%.4f\" % (step, loss_metric.result(),  m.result()))\n",
    "        \n",
    "    val_loss_metric = tf.keras.metrics.Mean()\n",
    "    val_acc = tf.keras.metrics.Accuracy()\n",
    "        \n",
    "    for i, (x_batch_test, y_batch_test) in enumerate(test_dataset):\n",
    "        #x_batch_test = tf.expand_dims(x_batch_test, axis=3)\n",
    "        pred_val = model(x_batch_test)                \n",
    "        val_loss = tf.reduce_mean(ce_loss_fn(y_batch_test, pred_val))\n",
    "        val_loss_metric(val_loss)\n",
    "        val_acc.update_state(tf.argmax(y_batch_test, 1),tf.argmax(pred_val, 1))\n",
    "    print(\"Validation: Mean loss = %.4f acc=%.4f\" % (val_loss_metric.result(),  val_acc.result()))                    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "#model.save_weights('./models/mnist_lenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on Target Domain (assuming NO labels available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here labels will be used only for evaluation of results NOT for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset usps\n",
      "Loading dataset usps\n",
      "Loading dataset mnist\n"
     ]
    }
   ],
   "source": [
    "usps_loader = DataLoader(\"usps\")\n",
    "X_tr, y_tr = usps_loader.getData('train')\n",
    "\n",
    "usps_loader = DataLoader(\"usps\")\n",
    "X_tr_test, y_tr_test = usps_loader.getData('test')\n",
    "\n",
    "mnist_loader = DataLoader(\"mnist\")\n",
    "X_sr, y_sr = mnist_loader.getData(\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Dataset Iterators for mini-batch training loop on target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_TR = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_dataset = tf.data.Dataset.from_tensor_slices((X_tr, y_tr))\n",
    "tgt_dataset = tgt_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE_TR)\n",
    "\n",
    "src_dataset = tf.data.Dataset.from_tensor_slices((X_sr, y_sr))\n",
    "src_dataset = src_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE_TR)\n",
    "\n",
    "tgt_dataset_test = tf.data.Dataset.from_tensor_slices((X_tr_test, y_tr_test))\n",
    "tgt_dataset_test = tgt_dataset_test.shuffle(buffer_size=1024).batch(BATCH_SIZE_TR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetModel = LeNet(training = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = tf.random.uniform(shape=(10, 28, 28, 1))\n",
    "y = targetModel(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.2952079   0.37245205 -0.03149784  0.37271476 -0.25152418\n",
      "   -0.3970836   0.02068714 -0.19946045 -0.38615113  0.2085048\n",
      "    0.15617841 -0.26530036 -0.37290442  0.08710288  0.46271327\n",
      "    0.19992392  0.48179376  0.04985712  0.22105566 -0.4961837 ]]\n",
      "\n",
      " [[ 0.3154899  -0.02073991  0.35343343 -0.08006986  0.16421616\n",
      "    0.53407896 -0.10781709  0.20271663  0.21474242 -0.3815879\n",
      "   -0.11711562  0.11146358  0.23061554 -0.4468818  -0.06483935\n",
      "    0.08043522 -0.49446213 -0.01319749 -0.18818846  0.15906008]]\n",
      "\n",
      " [[-0.0286163  -0.02196479 -0.4462118   0.01093923  0.07458931\n",
      "   -0.22513442  0.00879162  0.18701689  0.33425054 -0.15878253\n",
      "   -0.08912595 -0.20571202  0.15419707 -0.2604865  -0.02617104\n",
      "    0.00968052 -0.44290906 -0.06332861 -0.32153285 -0.00880899]]\n",
      "\n",
      " [[-0.01120962  0.19201182  0.20417696  0.18580563 -0.11774364\n",
      "   -0.2500204   0.4080259   0.25073114 -0.3796424   0.5824057\n",
      "    0.349913    0.5214375   0.5937191  -0.3679185   0.16061176\n",
      "    0.04294679  0.05984076 -0.4945863   0.61687005  0.02942169]]\n",
      "\n",
      " [[-0.4551311   0.39505246 -0.50625306 -0.26154736 -0.57579625\n",
      "    0.61418676 -0.2650236  -0.08322972 -0.08475977 -0.1641213\n",
      "   -0.45638704  0.29512605  0.28541207  0.04881446 -0.30868864\n",
      "   -0.04083183 -0.44185692 -0.3718813  -0.43418747 -0.01425856]]], shape=(5, 1, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(targetModel.encoder.layers[0].weights[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x20d5b81f388>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \".\\\\models\\\\mnist_lenet\"\n",
    "# init weights of target encoder with those of source encoder\n",
    "targetModel.load_weights(checkpoint_path)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if new weights loaded properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.07447814 -0.00880726 -0.2455518  -0.14313318 -0.206044\n",
      "    0.01054084 -0.45289755  0.39708784 -0.06829995  0.41723385\n",
      "    0.07748751 -0.42984262 -0.03139526 -0.11600942 -0.0976113\n",
      "    0.10573574 -0.37824258  0.02161951  0.5805371  -0.07381678]]\n",
      "\n",
      " [[-0.03983057  0.1241329   0.2847217  -0.24746835 -0.27586755\n",
      "   -0.05461131 -0.09643356 -0.09678437  0.19254015 -0.3950874\n",
      "   -0.20059995 -0.64347035 -0.6227867  -0.05109233  0.00334542\n",
      "    0.02967518 -0.3179615   0.0135891   0.24304534 -0.19977543]]\n",
      "\n",
      " [[-0.0078422   0.06580239 -0.3811191  -0.18182144  0.36455932\n",
      "   -0.0654147   0.314523    0.3750171   0.10549959 -0.09135032\n",
      "   -0.35417858 -0.27474433  0.2563004  -0.37529317 -0.10289144\n",
      "    0.42643112 -0.34611034  0.28337616 -0.27924997 -0.3395373 ]]\n",
      "\n",
      " [[ 0.2836141  -0.49414167 -0.13555625  0.09657162  0.15958536\n",
      "   -0.12617801  0.3260055  -0.02024609  0.6486877   0.13205835\n",
      "   -0.15898919 -0.12551579 -0.1586223   0.09797388 -0.5715533\n",
      "   -0.39811888  0.03213071 -0.00512786 -0.21663813 -0.57963556]]\n",
      "\n",
      " [[-0.05453404  0.17943753 -0.10631911  0.26644835 -0.06518687\n",
      "    0.29509267  0.04329489 -0.27711684  0.5233672   0.00302776\n",
      "   -0.00849182  0.2031403   0.11617143 -0.633115   -0.55202407\n",
      "   -0.4669164   0.12605356  0.31695834 -0.16204937 -0.42256343]]], shape=(5, 1, 20), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(targetModel.encoder.layers[0].weights[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Accuracy of Target Domain without any further training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8302016184336853\n"
     ]
    }
   ],
   "source": [
    "ypred = targetModel(X_tr)        \n",
    "predClass = tf.argmax(ypred, axis=1)\n",
    "print(accuracy_score(y_tr, predClass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8086696562032885\n"
     ]
    }
   ],
   "source": [
    "ypred = targetModel(X_tr_test)        \n",
    "predClass = tf.argmax(ypred, axis=1)\n",
    "print(accuracy_score(y_tr_test, predClass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Adaptation for Target Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load source model as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8302016184336853\n"
     ]
    }
   ],
   "source": [
    "model = LeNet(training=False)\n",
    "checkpoint_path = \".\\\\models\\\\mnist_lenet\"\n",
    "model.load_weights(checkpoint_path)         \n",
    "         \n",
    "ypred = model(X_tr)        \n",
    "predClass = tf.argmax(ypred, axis=1)\n",
    "print(accuracy_score(y_tr, predClass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             multiple                  250500    \n",
      "                                                                 \n",
      " dense_10 (Dense)            multiple                  250500    \n",
      "                                                                 \n",
      " dense_11 (Dense)            multiple                  501       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 501,501\n",
      "Trainable params: 501,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator(500)\n",
    "y_disc = discriminator(targetModel.encoder(sample_input))\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adverserial Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(source_output, target_output):\n",
    "    \n",
    "    pred = tf.concat([source_output, target_output], axis =0)\n",
    "    label = tf.concat([tf.ones_like(source_output), tf.zeros_like(target_output)], axis=0)\n",
    "    loss = cross_entropy(label, pred)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def generator_loss(target_output):\n",
    "    return tf.reduce_mean(cross_entropy(tf.ones_like(target_output), target_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrinator_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "targetEncoder_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init weights of target encoder with those of source encoder\n",
    "targetModel = LeNet()\n",
    "targetModel.load_weights(checkpoint_path)         \n",
    "targetEncoder = targetModel.encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adverserial Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "epoch 0 step 0 :Mean gen loss = 0.7130 Disc Loss=0.7669\n",
      "Target Acc:0.8087\n",
      "epoch 0 step 20 :Mean gen loss = 0.7219 Disc Loss=0.5748\n",
      "Target Acc:0.8084\n",
      "epoch 0 step 40 :Mean gen loss = 0.8231 Disc Loss=0.5093\n",
      "Target Acc:0.8102\n",
      "epoch 0 step 60 :Mean gen loss = 0.8723 Disc Loss=0.4818\n",
      "Target Acc:0.8130\n",
      "epoch 0 step 80 :Mean gen loss = 0.8882 Disc Loss=0.4713\n",
      "Target Acc:0.8151\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-187d35e0a8c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mdisc_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_disc_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_disc_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mgradients_of_D\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdisc_tape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisc_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mdiscrinator_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients_of_D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mdis_loss_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisc_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dl2\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dl2\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dl2\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dl2\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    594\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m           data_format=data_format)\n\u001b[0m\u001b[0;32m    597\u001b[0m   ]\n\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\dl2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_filter\u001b[1;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1081\u001b[0m         \u001b[1;34m\"strides\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"padding\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"explicit_paddings\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data_format\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1083\u001b[1;33m         data_format, \"dilations\", dilations)\n\u001b[0m\u001b[0;32m   1084\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "gen_loss_metric = tf.keras.metrics.Mean()\n",
    "dis_loss_metric = tf.keras.metrics.Mean()\n",
    "target_acc = tf.keras.metrics.Accuracy()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    gen_loss_metric.reset_states()\n",
    "    dis_loss_metric.reset_states()\n",
    "    target_acc.reset_states()\n",
    "    \n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "    tgt_dataset = tf.data.Dataset.from_tensor_slices((X_tr, y_tr))\n",
    "    tgt_dataset = tgt_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE_TR)\n",
    "\n",
    "    src_dataset = tf.data.Dataset.from_tensor_slices((X_sr, y_sr))\n",
    "    src_dataset = src_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE_TR)\n",
    "\n",
    "    tgt_dataset_test = tf.data.Dataset.from_tensor_slices((X_tr_test, y_tr_test))\n",
    "    tgt_dataset_test = tgt_dataset_test.shuffle(buffer_size=1024).batch(BATCH_SIZE_TR)\n",
    "    data_zip = enumerate(zip(src_dataset, tgt_dataset))\n",
    "    \n",
    "    for step, ((images_src, _), (images_tgt, y_tgt)) in data_zip:\n",
    "        #print(step, images_src.shape, images_tgt.shape)\n",
    "        if images_tgt.shape[0]<50:\n",
    "            break\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            #train discriminator\n",
    "            target_encoded = targetEncoder(images_tgt)\n",
    "            source_encoded = model.encoder(images_src)\n",
    "            \n",
    "            target_disc_pred = discriminator(target_encoded)\n",
    "            source_disc_pred = discriminator(source_encoded)\n",
    "            \n",
    "            disc_loss = discriminator_loss(source_disc_pred, target_disc_pred)\n",
    "        gradients_of_D = disc_tape.gradient(disc_loss, discriminator.trainable_weights)\n",
    "        discrinator_optimizer.apply_gradients(zip(gradients_of_D, discriminator.trainable_weights))\n",
    "        dis_loss_metric(disc_loss)\n",
    "        \n",
    "        with tf.GradientTape() as gen_tape:     \n",
    "            #train generator            \n",
    "            target_encoded = targetEncoder(images_tgt)\n",
    "            target_disc_pred = discriminator(target_encoded)\n",
    "            gen_loss = generator_loss(target_disc_pred)\n",
    "        \n",
    "        gradients_of_G = gen_tape.gradient(gen_loss, targetEncoder.trainable_weights)\n",
    "        targetEncoder_optimizer.apply_gradients(zip(gradients_of_G, targetEncoder.trainable_weights))            \n",
    "        gen_loss_metric(gen_loss)\n",
    "        \n",
    "            \n",
    "        if step % 20 == 0:\n",
    "            print(\"epoch %d step %d :Mean gen loss = %.4f Disc Loss=%.4f\" % (epoch, \n",
    "                                                                             step,\n",
    "                                                              gen_loss_metric.result(),\n",
    "                                                              dis_loss_metric.result()))\n",
    "                \n",
    "            for i, (x_batch_test, y_batch_test) in enumerate(tgt_dataset_test):\n",
    "                #test_predictions\n",
    "                pred = targetModel(x_batch_test)\n",
    "                predClass = tf.argmax(pred, axis=1)                    \n",
    "                target_acc.update_state(y_batch_test,predClass)\n",
    "            print(\"Target Acc:%.4f\" %(target_acc.result()))\n",
    "            \n",
    "    print(\"Epoch {} Acc{}\".format(epoch, target_acc.result()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#targetEncoder.save_weights('./models/target_encoder/adda_usps_lenet')                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of ADDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init weights of target encoder with those of source encoder\n",
    "targetModel = LeNet()\n",
    "targetModel.load_weights(checkpoint_path)         \n",
    "targetEncoder = targetModel.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x20d0402e788>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetEncoder.load_weights('./models/target_encoder/adda_usps_lenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Train Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Test Acc:0.9671\n"
     ]
    }
   ],
   "source": [
    "target_acc = tf.keras.metrics.Accuracy()\n",
    "for i, (x_batch_test, y_batch_test) in enumerate(tgt_dataset):\n",
    "    #train_predictions\n",
    "    pred = targetModel(x_batch_test)\n",
    "    predClass = tf.argmax(pred, axis=1)                    \n",
    "    target_acc.update_state(y_batch_test,predClass)\n",
    "print(\"Target Test Acc:%.4f\" %(target_acc.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Test Acc:0.9542\n"
     ]
    }
   ],
   "source": [
    "target_acc = tf.keras.metrics.Accuracy()\n",
    "for i, (x_batch_test, y_batch_test) in enumerate(tgt_dataset_test):\n",
    "    #test_predictions\n",
    "    pred = targetModel(x_batch_test)\n",
    "    predClass = tf.argmax(pred, axis=1)                    \n",
    "    target_acc.update_state(y_batch_test,predClass)\n",
    "print(\"Target Test Acc:%.4f\" %(target_acc.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
